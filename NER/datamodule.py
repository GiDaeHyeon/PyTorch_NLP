import torchfrom torch.utils.data import Dataset, DataLoaderfrom pytorch_lightning import LightningDataModulefrom transformers import BertTokenizerclass NERTokenizer(Dataset):    def __init__(self,                 pretrained_model_name: str = "kykim/bert-kor-base",                 data_directory: str = './dataset',                 filename: str = None,                 max_length: int = 256) -> None:        super(NERTokenizer, self).__init__()        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)        self.max_length = max_length        with open(f'{data_directory}/{filename}', 'r') as file:            data = file.readlines()        self.X, self.Y = [], []        if filename is None:            raise NotImplementedError('파일 지정 요망')        tokens = {'input_ids': [2],                  'token_type_ids': [0],                  'attention_mask': [1]}        label = ['O']        for d in data:            if d == '\n':                label.append('O')                tokens['input_ids'] += [3]                tokens['token_type_ids'] += [0]                tokens['attention_mask'] += [1]                self.X.append(tokens)                self.Y.append(label)                tokens = {'input_ids': [2],                          'token_type_ids': [0],                          'attention_mask': [1]}                label = []                continue            text, _, entity = d.split('\t')            entity = entity[:-1].split('_')[-1]            token = tokenizer.encode_plus(text,                                          add_special_tokens=False)            tokens['input_ids'] += token.input_ids            tokens['token_type_ids'] += token.token_type_ids            tokens['attention_mask'] += token.attention_mask            if entity == 'O':                for idx in range(len(token.input_ids)):                    if idx == 0:                        label.append('O')                    else:                        label.append('O')            else:                for idx in range(len(token.input_ids)):                    if idx == 0:                        label.append('B_' + entity)                    else:                        label.append('I_' + entity)        self.X.append(tokens)        self.Y.append(label)    def __getitem__(self, idx: int) -> [dict, list]:        token = self.X[idx]        label = self.Y[idx]        for num in range(self.max_length - len(label)):            token['input_ids'] += [0]            token['token_type_ids'] += [0]            token['attention_mask'] += [0]            label += ['O']        for key in token:            token[key] = torch.Tensor(token[key])        return token, label    def __len__(self):        return len(self.Y)class NERDataModule(LightningDataModule):    def __init__(self,                 max_length=256,                 batch_size=128,                 num_workers=4) -> None:        super(NERDataModule, self).__init__()        self.train_dataset = NERTokenizer(filename='ner.train',                                          max_length=max_length)        self.val_dataset = NERTokenizer(filename='ner.test',                                        max_length=max_length)        self.batch_size = batch_size        self.num_workers = num_workers    def train_dataloader(self) -> DataLoader:        return DataLoader(dataset=self.train_dataset,                          batch_size=self.batch_size,                          num_workers=self.num_workers,                          pin_memory=True,                          drop_last=True,                          shuffle=True)    def val_dataloader(self) -> DataLoader:        return DataLoader(dataset=self.val_dataset,                          batch_size=self.batch_size,                          num_workers=self.num_workers,                          pin_memory=True,                          drop_last=False,                          shuffle=False)