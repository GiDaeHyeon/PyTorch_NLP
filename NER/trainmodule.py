import torch.nn as nnimport torch.optim as optimfrom pytorch_lightning import LightningModulefrom torchmetrics import Accuracy, F1, Precision, Recallfrom model import BertCRFNERclass NERTrainModule(LightningModule):    def __init__(self,                 pretrained_model_name: str = 'kykim/bert-kor-base',                 freeze: bool = True,                 num_tags: int = 14,                 learning_rate: float = 1e-4):        super(NERTrainModule, self).__init__()        self.model = BertCRFNER(pretrained_model_name=pretrained_model_name,                                freeze=freeze,                                num_tags=num_tags)        self.acc = Accuracy()        self.f1 = F1(num_classes=num_tags)        self.precision = Precision(num_classes=num_tags)        self.recall = Recall(num_classes=num_tags)        self.learning_rate = learning_rate    def configure_optimizers(self):        return optim.AdamW(lr=self.learning_rate,                           params=self.model.parameters())    def forward(self, data, tags=None):        if tags is None:            return self.model(data)        elif tags is not None:            return self.model(data, tags)    def training_step(self, batch, batch_idx):        data, tags = batch        loss, sequence_of_tags = self(data, tags)        self.log('train_loss', loss, on_step=True, on_epoch=True)    def validation_step(self, data, tags):        data, tags = batch        loss, sequence_of_tags = self(data, tags)        self.acc(tags, sequence_of_tags)        self.f1(tags, sequence_of_tags)        self.precision(tags, sequence_of_tags)        self.recall(tags, sequence_of_tags)        self.log('val_loss', loss, on_step=False, on_epoch=True)        self.log('Accuracy', self.acc, on_step=False, on_epoch=True)        self.log('F1', self.f1, on_step=False, on_epoch=True)        self.log('Precision', self.precision, on_step=False, on_epoch=True)        self.log('Recall', self.recall, on_step=False, on_epoch=True)