import torchfrom torch.utils.data import Datasetimport refrom transformers import BertTokenizerFastimport pandas as pdclass BERTDataset(Dataset):    def __init__(self,                 data_dir=DATA_DIR,                 mode='train',                 max_len=MAX_LEN):        super().__init__()        self.tokenizer = BertTokenizerFast.from_pretrained(WEIGHT)        self.data = pd.read_csv(f'{data_dir}/{mode}.csv')        self.extract_data = ['과제명', '요약문_연구목표', '요약문_연구내용', '요약문_기대효과']        self.max_len = max_len        self.mode = mode    def __len__(self):        return len(self.data)    def preprocessing(self, item):        t = ''        for d in item[self.extract_data]:            t += str(d)        t = re.sub(r'\([^)]*\)', '', str(t))        t = re.sub(r'\n', '', str(t))        t = re.sub(r'[-=+,#/\?:^$.@*\"※~&%ㆍ!』\\‘|\(\)\[\]\<\>`\'…》○]', '', str(t))        t = t.replace('  ', '')        return str(t)    def preprocessing_for_bert(self, data):        encoded_sent = self.tokenizer.encode_plus(            text=self.preprocessing(data),            add_special_tokens=True,            max_length=self.max_len,            truncation=True,            pad_to_max_length=True,            return_attention_mask=True,            return_token_type_ids=True        )        input_ids = torch.tensor(encoded_sent.get('input_ids'))        attention_masks = torch.tensor(encoded_sent.get('attention_mask'))        token_type_ids = torch.tensor(encoded_sent.get('token_type_ids'))        return input_ids, attention_masks, token_type_ids    def __getitem__(self, idx):        item = self.data.loc[idx]        if self.mode == 'train':            return self.preprocessing_for_bert(item), torch.tensor(item['label'])        elif self.mode == 'test':            return self.preprocessing_for_bert(item)