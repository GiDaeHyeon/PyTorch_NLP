from torch.nn import Module, Embedding, LSTM, Linear, Dropout, LeakyReLU, Sequentialfrom torch.nn.functional import log_softmaxfrom transformers import BertModelclass BertModelForSentimentClassification(Module):    def __init__(self,                 weight=None,                 n_classes=None,                 freeze=True):        super().__init__()        self.model = BertModel.from_pretrained(weight)        if freeze:            for param in self.model.parameters():                param.require_grad = False        self.clf = Sequential(            Linear(768, 768),            Dropout(p=.3),            LeakyReLU(),            Linear(768, 256),            Dropout(p=.3),            LeakyReLU(),            Linear(256, n_classes)        )    def forward(self, input_ids, attention_mask, token_type_ids):        x = self.model(input_ids=input_ids,                       attention_mask=attention_mask,                       token_type_ids=token_type_ids)        x = self.clf(x.pooler_output)        return log_softmax(x)class BaseLineModelForSentimentClassification(Module):    def __init__(self,                 vocab_size=None,                 wordvec_dim=None,                 hidden_size=None,                 num_layers=4,                 dropout_p=.3,                 bidirectional=True):        self.Embedding = Embedding(vocab_size, wordvec_dim)        self.lstm = LSTM(input_size=wordvec_dim,                         hidden_size=hidden_size,                         num_layers=num_layers,                         dropout=dropout_p,                         batch_first=True,                         bidirectional=bidirectional)        if bidirectional:            hidden_size *= 2        self.clf = Sequential(            Linear(768, 768),            Dropout(p=.3),            LeakyReLU(),            Linear(768, 256),            Dropout(p=.3),            LeakyReLU(),            Linear(256, 2)        )    def forward(self, x):        wordvec = self.Embedding(x)        logit, _ = self.lstm(wordvec)        y_hat = log_softmax(logit[:, -1])        return y_hat